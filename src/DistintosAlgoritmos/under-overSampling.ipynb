{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centrar los datos, under sampling y over sampling##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import SCORERS\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = pd.read_csv('featuresMagui.csv') #mis features \n",
    "tf_seba = pd.read_csv('features_seba.csv') #los features de Seba\n",
    "tf_santi = pd.read_csv('santi_timefeatures.csv')\n",
    "tf_santi_2 = pd.read_csv('Santi_FeaturesConEventos.csv')\n",
    "labels = pd.read_csv('../data/labels_training_set.csv') #las personas de las cuales tengo Info\n",
    "personas =pd.read_csv('../data/trocafone_kaggle_test.csv') #las personas a las que le tengo que predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datos a entregar\n",
    "datos = pd.merge(tf, tf_seba, on = 'person', how = 'inner')\n",
    "#datos = pd.merge(datos, tf_santi, on = 'person', how = 'inner')\n",
    "#datos = pd.merge(datos, tf_santi_2, on = 'person', how = 'inner')\n",
    "datos = pd.merge(personas, tf, on = 'person', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datos para entrenar\n",
    "#labels_f = pd.merge(tf,tf_seba, on = 'person',how = 'inner')\n",
    "#labels_f = pd.merge(labels_f,tf_santi, on = 'person',how = 'inner')\n",
    "#labels_f = pd.merge(labels_f,tf_santi_2, on = 'person',how = 'inner')\n",
    "labels_f = pd.merge(labels, tf, on = 'person', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas = labels_f.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['person', 'label', 'Unnamed: 0', 'esNuevo', 'cantidadDeVisitas',\n",
       "       'ingresoPorPublicidad', 'Facebook', 'FacebookAds', 'FacebookSocial',\n",
       "       'MARKETING SOCIAL',\n",
       "       ...\n",
       "       'ingresoXgoogle', 'ingresoXindexa', 'ingresoXmanifest',\n",
       "       'ingresoXmercadopago', 'ingresoXonsite', 'ingresoXrakuten',\n",
       "       'ingresoXrtbhouse', 'ingresoXvoxus', 'ingresoXyotpo', 'ingresoXzanox'],\n",
       "      dtype='object', length=107)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defino Xgboost en un solo, lo que hace es entrenar predecir e imprimir las predicciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost(X_p, y_p, X_train_p, X_test_p, y_train_p, y_test_p):\n",
    "    xg_reg = xgb.XGBClassifier(learning_rate =0.05,n_estimators=1000,max_depth=5,min_child_weight=1,gamma=0,\n",
    "                           subsample=0.8,colsample_bytree=0.8,objective= 'binary:logistic',nthread=4,\n",
    "                            seed=27)\n",
    "    xg_reg.fit(X_train_p,y_train_p, early_stopping_rounds = 5, eval_set=[(X_test, y_test)])\n",
    "    preds = xg_reg.predict(X_test_p)\n",
    "    preds_prob = xg_reg.predict_proba(X_test_p)[:,1]\n",
    "    train_accuracy = accuracy_score(y_train_p, xg_reg.predict(X_train_p))\n",
    "    test_accuracy = accuracy_score(y_test_p, preds)\n",
    "    area_debajo_de_curva = roc_auc_score(y_test_p, preds_prob)\n",
    "    matriz_de_confusion = confusion_matrix(y_test_p, preds)\n",
    "    print('train acurracy: ')\n",
    "    print(train_accuracy)\n",
    "    print('test acurracy: ')\n",
    "    print(test_accuracy)\n",
    "    print('Matriz de confusión: ')\n",
    "    print(matriz_de_confusion)\n",
    "    print('Área bajo la curva: ')\n",
    "    print(area_debajo_de_curva)\n",
    "    return(xg_reg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['person', 'label', 'Unnamed: 0', 'esNuevo', 'cantidadDeVisitas',\n",
      "       'ingresoPorPublicidad', 'Facebook', 'FacebookAds', 'FacebookSocial',\n",
      "       'MARKETING SOCIAL',\n",
      "       ...\n",
      "       'ingresoXgoogle', 'ingresoXindexa', 'ingresoXmanifest',\n",
      "       'ingresoXmercadopago', 'ingresoXonsite', 'ingresoXrakuten',\n",
      "       'ingresoXrtbhouse', 'ingresoXvoxus', 'ingresoXyotpo', 'ingresoXzanox'],\n",
      "      dtype='object', length=107)\n"
     ]
    }
   ],
   "source": [
    "print(columnas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Por un lado tengo los features y por otro los labels. \n",
    "labels_f.reset_index()\n",
    "\n",
    "y = labels_f.iloc[:,1:2]\n",
    "X = labels_f.iloc[:,3: 107]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('device_type', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for columna in X.columns: \n",
    " #   print(columna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplico XGboost solo, es decir, **sin ninguna mejora **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.048919\n",
      "Will train until validation_0-error hasn't improved in 5 rounds.\n",
      "[1]\tvalidation_0-error:0.049434\n",
      "[2]\tvalidation_0-error:0.047889\n",
      "[3]\tvalidation_0-error:0.047889\n",
      "[4]\tvalidation_0-error:0.047889\n",
      "[5]\tvalidation_0-error:0.048404\n",
      "[6]\tvalidation_0-error:0.047889\n",
      "[7]\tvalidation_0-error:0.047889\n",
      "Stopping. Best iteration:\n",
      "[2]\tvalidation_0-error:0.047889\n",
      "\n",
      "train acurracy: \n",
      "0.9503205128205128\n",
      "test acurracy: \n",
      "0.9521112255406797\n",
      "Matriz de confusión: \n",
      "[[1848    0]\n",
      " [  93    1]]\n",
      "Área bajo la curva: \n",
      "0.7870757345491388\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.1, random_state= 123)\n",
    "\n",
    "xg_reg =xgboost(X, y, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos de la matriz de confusión son **ALARMANTES** ya que predice mal los que compran. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trato de centrar los datos con el **escalador estandar** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype bool, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype bool, float64 were all converted to float64 by StandardScaler.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.048919\n",
      "Will train until validation_0-error hasn't improved in 5 rounds.\n",
      "[1]\tvalidation_0-error:0.049434\n",
      "[2]\tvalidation_0-error:0.047889\n",
      "[3]\tvalidation_0-error:0.047889\n",
      "[4]\tvalidation_0-error:0.047889\n",
      "[5]\tvalidation_0-error:0.048404\n",
      "[6]\tvalidation_0-error:0.047889\n",
      "[7]\tvalidation_0-error:0.047889\n",
      "Stopping. Best iteration:\n",
      "[2]\tvalidation_0-error:0.047889\n",
      "\n",
      "train acurracy: \n",
      "0.9503205128205128\n",
      "test acurracy: \n",
      "0.9521112255406797\n",
      "Matriz de confusión: \n",
      "[[1848    0]\n",
      " [  93    1]]\n",
      "Área bajo la curva: \n",
      "0.7870757345491388\n"
     ]
    }
   ],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X_transformed = scaler.transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size= 0.1, random_state= 123)\n",
    "\n",
    "xg_reg =xgboost(X_transformed,y,X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trato de centrar los datos con el escalador **MaxAbsScaler **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.048919\n",
      "Will train until validation_0-error hasn't improved in 5 rounds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalidation_0-error:0.049434\n",
      "[2]\tvalidation_0-error:0.047889\n",
      "[3]\tvalidation_0-error:0.047889\n",
      "[4]\tvalidation_0-error:0.047889\n",
      "[5]\tvalidation_0-error:0.048404\n",
      "[6]\tvalidation_0-error:0.047889\n",
      "[7]\tvalidation_0-error:0.047889\n",
      "Stopping. Best iteration:\n",
      "[2]\tvalidation_0-error:0.047889\n",
      "\n",
      "train acurracy: \n",
      "0.9503205128205128\n",
      "test acurracy: \n",
      "0.9521112255406797\n",
      "Matriz de confusión: \n",
      "[[1848    0]\n",
      " [  93    1]]\n",
      "Área bajo la curva: \n",
      "0.7870987611679101\n"
     ]
    }
   ],
   "source": [
    "scaler = preprocessing.MaxAbsScaler().fit(X)\n",
    "X_transformed = scaler.transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size= 0.1, random_state= 123)\n",
    "\n",
    "xg_reg =xgboost(X_transformed, y, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El escalador MaxAbsScales es mejor que el estandar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En función de los datos obtenidos de la matriz de confusión, nos dimos cuenta que estaba dando muchos falsos negativos de los que compraron. Por lo tanto, recurriremos a una técnica de **UNDER SAMPLING** y luego a otra técnica de **OVER SAMPLING** y luego compararemos los resultados obtenidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uso otra manera de hacer over\\under sampling que es mediante la librería **imbalanced-learn** de python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFO: \n",
    "\n",
    "For example, we can cluster the records of the majority class, and do the under-sampling by removing records from each cluster, thus seeking to preserve information. In over-sampling, instead of creating exact copies of the minority class records, we can introduce small variations into those copies, creating more diverse synthetic samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNDER SAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.00340136 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.00680272 1.         ... 0.         0.         0.        ]\n",
      " [1.         0.00340136 1.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.03401361 1.         ... 0.         0.         0.        ]\n",
      " [0.         0.02040816 1.         ... 0.         0.         0.        ]\n",
      " [0.         0.03741497 1.         ... 0.         0.         0.        ]]\n",
      "[0]\tvalidation_0-error:0.273944\n",
      "Will train until validation_0-error hasn't improved in 5 rounds.\n",
      "[1]\tvalidation_0-error:0.260041\n",
      "[2]\tvalidation_0-error:0.268795\n",
      "[3]\tvalidation_0-error:0.268795\n",
      "[4]\tvalidation_0-error:0.261586\n",
      "[5]\tvalidation_0-error:0.263131\n",
      "[6]\tvalidation_0-error:0.261071\n",
      "Stopping. Best iteration:\n",
      "[1]\tvalidation_0-error:0.260041\n",
      "\n",
      "train acurracy: \n",
      "0.7765237020316027\n",
      "test acurracy: \n",
      "0.7399588053553038\n",
      "Matriz de confusión: \n",
      "[[1368  480]\n",
      " [  25   69]]\n",
      "Área bajo la curva: \n",
      "0.8121977756286267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rus= RandomUnderSampler(return_indices=True)\n",
    "\n",
    "#id_rus son los índices\n",
    "X_rus_train, y_rus_train, id_rus = rus.fit_sample(X_train, y_train)\n",
    "print(X_rus_train)\n",
    "xg_reg = xgboost(X_transformed, y ,X_rus_train, X_test, y_rus_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_f = datos.iloc[:,2:406]\n",
    "#print(datos_f)\n",
    "datos_f = datos_f.drop('device_type', axis= 1)\n",
    "scaler = preprocessing.MaxAbsScaler().fit(datos_f)\n",
    "datos_f = scaler.transform(datos_f)\n",
    "#print(datos_f)\n",
    "preds_entrega = xg_reg.predict_proba(datos_f)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5122923 , 0.53299916, 0.540401  , ..., 0.5122923 , 0.5122923 ,\n",
       "       0.5122923 ], dtype=float32)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_entrega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(preds_entrega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = datos[['person']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['label'] = preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         person     label\n",
      "0      4886f805  0.512292\n",
      "1      0297fc1e  0.532999\n",
      "2      2d681dd8  0.540401\n",
      "3      cccea85e  0.528378\n",
      "4      4c8a8b93  0.521840\n",
      "5      29ebb414  0.529574\n",
      "6      3dc1950f  0.521840\n",
      "7      8ea4c165  0.521840\n",
      "8      d8cfe234  0.470360\n",
      "9      d6bc64df  0.521840\n",
      "10     ec9c4059  0.495050\n",
      "11     d21b8e6b  0.466749\n",
      "12     2a724d87  0.512292\n",
      "13     686c49c9  0.540401\n",
      "14     a08d42ea  0.503667\n",
      "15     c98f5cf1  0.483674\n",
      "16     d614c608  0.522364\n",
      "17     e45acd53  0.532999\n",
      "18     7d876393  0.510818\n",
      "19     f5af843f  0.518315\n",
      "20     5a724794  0.528378\n",
      "21     00091926  0.510818\n",
      "22     55d1e0ee  0.498126\n",
      "23     f87be219  0.512292\n",
      "24     49c19e32  0.470360\n",
      "25     bb78c182  0.512292\n",
      "26     e2bfe05f  0.512292\n",
      "27     0e9d0ae2  0.530869\n",
      "28     cb68850c  0.525430\n",
      "29     f30ef764  0.532634\n",
      "...         ...       ...\n",
      "19385  523c7e69  0.512292\n",
      "19386  d2e564cb  0.512292\n",
      "19387  eb27e544  0.521856\n",
      "19388  b32e7113  0.512292\n",
      "19389  a65f2cf0  0.512292\n",
      "19390  a70b9f00  0.525028\n",
      "19391  a161fd76  0.512292\n",
      "19392  e9a4d3a8  0.521856\n",
      "19393  24f53ba2  0.512292\n",
      "19394  39cf8fa0  0.512292\n",
      "19395  9ceab28a  0.520812\n",
      "19396  f85da107  0.512292\n",
      "19397  154d2935  0.512292\n",
      "19398  3adf7ca4  0.525028\n",
      "19399  2e89874a  0.512292\n",
      "19400  ef4e52ab  0.512292\n",
      "19401  85e0f62a  0.525028\n",
      "19402  2c209f87  0.512292\n",
      "19403  4ddb8c19  0.512292\n",
      "19404  25bd8078  0.506576\n",
      "19405  87d306fc  0.512292\n",
      "19406  a2b1e355  0.512292\n",
      "19407  fb88a7ea  0.512292\n",
      "19408  9707cd0e  0.521856\n",
      "19409  6f7632db  0.512292\n",
      "19410  a1c2a901  0.512292\n",
      "19411  ed3f80d7  0.521856\n",
      "19412  92f2d94b  0.512292\n",
      "19413  40bf23ab  0.512292\n",
      "19414  80aea0a0  0.512292\n",
      "\n",
      "[19415 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('predicciones_magui.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OVER SAMPLING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.301236\n",
      "Will train until validation_0-error hasn't improved in 5 rounds.\n",
      "[1]\tvalidation_0-error:0.297116\n",
      "[2]\tvalidation_0-error:0.279094\n",
      "[3]\tvalidation_0-error:0.284243\n",
      "[4]\tvalidation_0-error:0.259526\n",
      "[5]\tvalidation_0-error:0.260041\n",
      "[6]\tvalidation_0-error:0.257467\n",
      "[7]\tvalidation_0-error:0.257467\n",
      "[8]\tvalidation_0-error:0.253347\n",
      "[9]\tvalidation_0-error:0.246138\n",
      "[10]\tvalidation_0-error:0.246653\n",
      "[11]\tvalidation_0-error:0.245623\n",
      "[12]\tvalidation_0-error:0.248713\n",
      "[13]\tvalidation_0-error:0.251287\n",
      "[14]\tvalidation_0-error:0.251287\n",
      "[15]\tvalidation_0-error:0.251287\n",
      "[16]\tvalidation_0-error:0.249228\n",
      "Stopping. Best iteration:\n",
      "[11]\tvalidation_0-error:0.245623\n",
      "\n",
      "train acurracy: \n",
      "0.7839442903653684\n",
      "test acurracy: \n",
      "0.7543769309989702\n",
      "Matriz de confusión: \n",
      "[[1398  450]\n",
      " [  27   67]]\n",
      "Área bajo la curva: \n",
      "0.818697038776826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learning_rate=0.05, max_delta_step=0,\n",
       "       max_depth=5, min_child_weight=1, missing=None, n_estimators=1000,\n",
       "       n_jobs=1, nthread=4, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=27, silent=True,\n",
       "       subsample=0.8)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ros = RandomOverSampler()\n",
    "X_ros_train, y_ros_train = ros.fit_sample(X_train, y_train)\n",
    "\n",
    "xgboost(X_transformed, y, X_ros_train, X_test, y_ros_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uso otra manera de hacer Undersample (que estuvo dando buenos resultados) con la siguiente técnica **Under-sampling: Tomek links **\n",
    "\n",
    "Info:\n",
    "\n",
    "Tomek links are pairs of very close instances, but of opposite classes. Removing the instances of the majority class of each pair increases the space between the two classes, facilitating the classification process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.049949\n",
      "Will train until validation_0-error hasn't improved in 5 rounds.\n",
      "[1]\tvalidation_0-error:0.048919\n",
      "[2]\tvalidation_0-error:0.049434\n",
      "[3]\tvalidation_0-error:0.048404\n",
      "[4]\tvalidation_0-error:0.049434\n",
      "[5]\tvalidation_0-error:0.048919\n",
      "[6]\tvalidation_0-error:0.048919\n",
      "[7]\tvalidation_0-error:0.048404\n",
      "[8]\tvalidation_0-error:0.046859\n",
      "[9]\tvalidation_0-error:0.048919\n",
      "[10]\tvalidation_0-error:0.047889\n",
      "[11]\tvalidation_0-error:0.047374\n",
      "[12]\tvalidation_0-error:0.047374\n",
      "[13]\tvalidation_0-error:0.046859\n",
      "Stopping. Best iteration:\n",
      "[8]\tvalidation_0-error:0.046859\n",
      "\n",
      "train acurracy: \n",
      "0.9529840388619014\n",
      "test acurracy: \n",
      "0.9531410916580845\n",
      "Matriz de confusión: \n",
      "[[1845    3]\n",
      " [  88    6]]\n",
      "Área bajo la curva: \n",
      "0.8292288385373492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learning_rate=0.05, max_delta_step=0,\n",
       "       max_depth=5, min_child_weight=1, missing=None, n_estimators=1000,\n",
       "       n_jobs=1, nthread=4, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=27, silent=True,\n",
       "       subsample=0.8)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl = TomekLinks(return_indices=True)#, ratio='majority')\n",
    "\n",
    "X_tl_train, y_tl_train, id_tl = tl.fit_sample(X_train, y_train)\n",
    "\n",
    "xgboost(X_transformed, y, X_tl_train, X_test, y_tl_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uso otra manera de hacer Under sample que es **Cluster Centroids**.\n",
    "\n",
    "INFO:\n",
    "\n",
    "This technique performs under-sampling by generating centroids based on clustering methods. The data will be previously grouped by similarity, in order to preserve information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.945932\n",
      "Will train until validation_0-error hasn't improved in 5 rounds.\n",
      "[1]\tvalidation_0-error:0.949022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\tvalidation_0-error:0.949022\n",
      "[3]\tvalidation_0-error:0.949022\n",
      "[4]\tvalidation_0-error:0.949537\n",
      "[5]\tvalidation_0-error:0.949537\n",
      "Stopping. Best iteration:\n",
      "[0]\tvalidation_0-error:0.945932\n",
      "\n",
      "train acurracy: \n",
      "0.9864559819413092\n",
      "test acurracy: \n",
      "0.054067971163748715\n",
      "Matriz de confusión: \n",
      "[[  11 1837]\n",
      " [   0   94]]\n",
      "Área bajo la curva: \n",
      "0.49943584784010314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learning_rate=0.05, max_delta_step=0,\n",
       "       max_depth=5, min_child_weight=1, missing=None, n_estimators=1000,\n",
       "       n_jobs=1, nthread=4, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=27, silent=True,\n",
       "       subsample=0.8)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc = ClusterCentroids()#ratio={0: 10})\n",
    "X_cc_train, y_cc_train = cc.fit_sample(X_train, y_train)\n",
    "\n",
    "\n",
    "xgboost(X_transformed, y, X_cc_train, X_test, y_cc_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra manera de hacer Over Sampling es **SMOTE**.\n",
    "\n",
    "Info: \n",
    "\n",
    "SMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class, based on those that already exist. It works randomly picking a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.122039\n",
      "Will train until validation_0-error hasn't improved in 5 rounds.\n",
      "[1]\tvalidation_0-error:0.121524\n",
      "[2]\tvalidation_0-error:0.125644\n",
      "[3]\tvalidation_0-error:0.121524\n",
      "[4]\tvalidation_0-error:0.119464\n",
      "[5]\tvalidation_0-error:0.121009\n",
      "[6]\tvalidation_0-error:0.11792\n",
      "[7]\tvalidation_0-error:0.11586\n",
      "[8]\tvalidation_0-error:0.1138\n",
      "[9]\tvalidation_0-error:0.114315\n",
      "[10]\tvalidation_0-error:0.112255\n",
      "[11]\tvalidation_0-error:0.1138\n",
      "[12]\tvalidation_0-error:0.11277\n",
      "[13]\tvalidation_0-error:0.11277\n",
      "[14]\tvalidation_0-error:0.110711\n",
      "[15]\tvalidation_0-error:0.109681\n",
      "[16]\tvalidation_0-error:0.110196\n",
      "[17]\tvalidation_0-error:0.109681\n",
      "[18]\tvalidation_0-error:0.107621\n",
      "[19]\tvalidation_0-error:0.107106\n",
      "[20]\tvalidation_0-error:0.108136\n",
      "[21]\tvalidation_0-error:0.106591\n",
      "[22]\tvalidation_0-error:0.105046\n",
      "[23]\tvalidation_0-error:0.104531\n",
      "[24]\tvalidation_0-error:0.107621\n",
      "[25]\tvalidation_0-error:0.106591\n",
      "[26]\tvalidation_0-error:0.106591\n",
      "[27]\tvalidation_0-error:0.105046\n",
      "[28]\tvalidation_0-error:0.106076\n",
      "Stopping. Best iteration:\n",
      "[23]\tvalidation_0-error:0.104531\n",
      "\n",
      "train acurracy: \n",
      "0.9309658748341976\n",
      "test acurracy: \n",
      "0.8954685890834192\n",
      "Matriz de confusión: \n",
      "[[1697  151]\n",
      " [  52   42]]\n",
      "Área bajo la curva: \n",
      "0.8288661692917012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learning_rate=0.05, max_delta_step=0,\n",
       "       max_depth=5, min_child_weight=1, missing=None, n_estimators=1000,\n",
       "       n_jobs=1, nthread=4, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=27, silent=True,\n",
       "       subsample=0.8)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smote = SMOTE()#ratio='minority')\n",
    "X_sm_train, y_sm_train = smote.fit_sample(X_train, y_train)\n",
    "\n",
    "xgboost(X_transformed, y, X_sm_train, X_test, y_sm_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La última manera que voy a realizar sampling es una **combinación entre SMOTE y Tomek Link**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.127703\n",
      "Will train until validation_0-error hasn't improved in 5 rounds.\n",
      "[1]\tvalidation_0-error:0.129248\n",
      "[2]\tvalidation_0-error:0.117405\n",
      "[3]\tvalidation_0-error:0.110711\n",
      "[4]\tvalidation_0-error:0.11586\n",
      "[5]\tvalidation_0-error:0.114315\n",
      "[6]\tvalidation_0-error:0.110196\n",
      "[7]\tvalidation_0-error:0.115345\n",
      "[8]\tvalidation_0-error:0.113285\n",
      "[9]\tvalidation_0-error:0.119979\n",
      "[10]\tvalidation_0-error:0.11483\n",
      "[11]\tvalidation_0-error:0.11895\n",
      "Stopping. Best iteration:\n",
      "[6]\tvalidation_0-error:0.110196\n",
      "\n",
      "train acurracy: \n",
      "0.9189075123598215\n",
      "test acurracy: \n",
      "0.8898043254376931\n",
      "Matriz de confusión: \n",
      "[[1683  165]\n",
      " [  49   45]]\n",
      "Área bajo la curva: \n",
      "0.818717187068251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learning_rate=0.05, max_delta_step=0,\n",
       "       max_depth=5, min_child_weight=1, missing=None, n_estimators=1000,\n",
       "       n_jobs=1, nthread=4, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=27, silent=True,\n",
       "       subsample=0.8)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smt = SMOTETomek(ratio='auto')\n",
    "X_smt_train, y_smt_train = smt.fit_sample(X_train, y_train)\n",
    "\n",
    "xgboost(X_transformed, y, X_smt_train, X_test, y_smt_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para hacer la entrega en Kaggle ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_f = datos.loc[:, columnas]\n",
    "scaler = preprocessing.MaxAbsScaler().fit(datos_f)\n",
    "datos_f = scaler.transform(datos_f)\n",
    "preds_entrega = xg_reg.predict_proba(datos_f)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99993944, 0.9332026 , 0.99999094, ..., 0.15517399, 0.05298465,\n",
       "       0.05298465], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_entrega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(preds_entrega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = datos[['person']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['label'] = preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('predicciones_magui.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESTO DE ACÁ ABAJO NO ANDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X.info())\n",
    "X['esNuevo']= X['esNuevo'].astype(int)\n",
    "X['adCampaignHit']= X['adCampaignHit'].astype(int)           \n",
    "X['bandListing']= X['bandListing'].astype(int)              \n",
    "X['checkout']=   X['checkout'].astype(int)                \n",
    "X['genericListing']=  X['genericListing'].astype(int)       \n",
    "X['searchEngineHit']= X['searchEngineHit'].astype(int)         \n",
    "X['searchedProducts']= X['searchedProducts'].astype(int)          \n",
    "X['staticpage']= X['staticpage'].astype(int)               \n",
    "X['viewedProduct']= X['viewedProduct'].astype(int)             \n",
    "X['visitedSite']= X['visitedSite'].astype(int)               \n",
    "X['deviceSmartphone']= X['deviceSmartphone'].astype(int)          \n",
    "X['deviceComputer']= X['deviceComputer'].astype(int)           \n",
    "X['deviceTablet']= X['deviceTablet'].astype(int)              \n",
    "X['deviceUnknown']= X['deviceUnknown'].astype(int)             \n",
    "X['visitaALaManiana']= X['visitaALaManiana'].astype(int)         \n",
    "X['visitaALaNoche']= X['visitaALaNoche'].astype(int)          \n",
    "X['visitaALaTarde']=  X['visitaALaTarde'].astype(int)           \n",
    "X['visitaALaMadrugada']= X['visitaALaMadrugada'].astype(int)\n",
    "#print(X.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decido usar el MaxAbsScaler\n",
    "scaler = preprocessing.MaxAbsScaler().fit(X)\n",
    "X_transformed = scaler.transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size= 0.1, random_state= 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'label_x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c20b53a4786a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#UNDER SAMPLING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#cuánta información hay de cada clase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcount_class_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_class_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_class_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_class_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'label_x'"
     ]
    }
   ],
   "source": [
    "#UNDER SAMPLING\n",
    "#cuánta información hay de cada clase \n",
    "count_class_0, count_class_1 = X_train.label_x.value_counts()\n",
    "\n",
    "print(count_class_0, count_class_1)\n",
    "# divido por clase la información\n",
    "label_0 = X_train[X_train['label_x'] == 0]\n",
    "label_1 = X_train[X_train['label_x'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_0_under = label_0.sample(count_class_1)\n",
    "label_f_under = pd.concat([label_0_under, label_1], axis=0)\n",
    "\n",
    "X_train_u = label_f_under.loc[:,columnas]\n",
    "y_train_u = label_f_under.loc[:,['label_x']]\n",
    "\n",
    "\n",
    "xgboost(X_transformed, y, X_train_u, X_test, y_train_u, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OVER SAMPLING\n",
    "count_class_0, count_class_1 = X_train.label_x.value_counts()\n",
    "\n",
    "label_0 = X_train[X_train['label_x'] == 0]\n",
    "label_1 = X_train[X_train['label_x'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_1_over = label_1.sample(count_class_0, replace=True)\n",
    "label_f_over = pd.concat([label_0, label_1_over], axis=0)\n",
    "\n",
    "X_train_o = X_train.loc[:,columnas]\n",
    "y_train_o = X_train.loc[:,['label_x']]\n",
    "\n",
    "xgboost(X_transformed, y, X_train_o, X_test, y_train_o, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver usando **Under sampling, mejoraron los resultados** teniendo en cuenta la matriz de confusión, pero **Over sampling** es mucho mejor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
