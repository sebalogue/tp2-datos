{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebo Stacking (Stratified) con los datos filtrados#\n",
    "\n",
    "El stack consiste de los siguientes modelos como 1º paso:\n",
    "\n",
    "    RandomForestClassifier\n",
    "\n",
    "    ExtraTreesClassifier\n",
    "\n",
    "    AdaBoostClassifier\n",
    "\n",
    "    GradientBoostingClassifier\n",
    "    \n",
    "    XGBClassifier\n",
    "    \n",
    "Y como 2º paso MLPClassifier\n",
    "\n",
    "Uso Random under sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in our libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import SCORERS\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Going to use these 5 base models for the stacking\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargo los Features###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_f = pd.read_csv('../../data/labels_f_filtrado.csv')\n",
    "datos = pd.read_csv('../../data/datos_filtrado.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "atributos = labels_f.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "atributos.remove('person')\n",
    "atributos.remove('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = labels_f['label'].ravel()\n",
    "X = labels_f.loc[:,atributos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MaxAbsScaler().fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca = PCA(n_components=10).fit(X)\n",
    "#X = pca.transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicarSmote(X_train, y_train):\n",
    "    smote = SMOTE(ratio='minority')\n",
    "    X_smt_train, y_smt_train = smote.fit_sample(X_train, y_train)\n",
    "    return (X_smt_train, y_smt_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicarRus(X_train_p, y_train_p):\n",
    "    rus= RandomUnderSampler(return_indices=True)\n",
    "    #id_rus son los índices\n",
    "    X_rus_train, y_rus_train, id_rus = rus.fit_sample(X_train_p, y_train_p)\n",
    "    return(X_rus_train, y_rus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.1, random_state= 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = aplicarRus(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = X_train.shape[0]\n",
    "ntest = X_test.shape[0]\n",
    "SEED = 0 # for reproducibility\n",
    "NFOLDS = 5 # set folds for out-of-fold prediction\n",
    "class SklearnHelper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict_proba(x)[:,1]\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        return self.clf.fit(x,y)\n",
    "    \n",
    "    def feature_importances(self,x,y):\n",
    "        print(self.clf.fit(x,y).feature_importances_)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    \\n    for i,(train_index, test_index) in enumerate(kf.split(X_transformed)):\\n        x_train, x_test = X_transformed[train_index], X_transformed[test_index]\\n        y_train, y_test = y[train_index], y[test_index]\\n        try:\\n            x_tr = x_train[train_index]\\n            y_tr = y_train[train_index]\\n            x_te = x_train[test_index]\\n            \\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"    \n",
    "    for i,(train_index, test_index) in enumerate(kf.split(X_transformed)):\n",
    "        x_train, x_test = X_transformed[train_index], X_transformed[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        try:\n",
    "            x_tr = x_train[train_index]\n",
    "            y_tr = y_train[train_index]\n",
    "            x_te = x_train[test_index]\n",
    "            \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clf, x_train_p, y_train_p, x_test_p):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits= NFOLDS, random_state=SEED)\n",
    "    kf = kf.split(x_train_p,y_train_p)\n",
    "    i = 0\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train_p[train_index]\n",
    "        y_tr = y_train_p[train_index]\n",
    "        x_te = x_train_p[test_index]\n",
    "        clf.fit(x_tr, y_tr)\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test_p)\n",
    "        i+=1\n",
    "        #print(i)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {'bootstrap': True, 'class_weight': None, 'criterion': 'gini',\n",
    "            'max_depth': 9, 'max_features': 'auto', 'max_leaf_nodes': None,\n",
    "            'min_impurity_decrease': 0.0, 'min_impurity_split': None,\n",
    "            'min_samples_leaf': 1, 'min_samples_split': 2,\n",
    "            'min_weight_fraction_leaf': 0.0, 'n_estimators': 1444, 'n_jobs': None,\n",
    "            'oob_score': False, 'verbose': 0,\n",
    "            'warm_start': False\n",
    "}#'n_jobs': -1,\n",
    "\n",
    "# Extra Trees Parameters\n",
    "et_params = {'bootstrap': False, 'class_weight': None, 'criterion': 'gini',\n",
    "          'max_depth': 100, 'max_features': 'auto','max_leaf_nodes': None,\n",
    "           'min_impurity_decrease': 0.0, 'min_impurity_split': None,\n",
    "           'min_samples_leaf': 5, 'min_samples_split': 5,\n",
    "           'min_weight_fraction_leaf': 0.0, 'n_estimators': 200,'n_jobs': 4}\n",
    "\n",
    "# AdaBoost parameters\n",
    "ada_params = {'algorithm': 'SAMME.R', 'base_estimator': None,\n",
    "          'learning_rate': 0.01, 'n_estimators': 977, 'random_state': None}\n",
    "\n",
    "# Gradient Boosting parameters\n",
    "gb_params = {\n",
    "    'n_estimators': 500,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "xgb_params = {'colsample_bylevel': 0.8805332918491438, 'colsample_bytree': 0.9352504152264935, 'gamma': 9.881437744998433e-06, 'learning_rate': 0.19933863096197899, 'max_delta_step': 6, 'max_depth': 18, 'min_child_weight': 3, 'n_estimators': 61, 'reg_alpha': 7.871401820904661e-05, 'reg_lambda': 1000.0, 'scale_pos_weight': 5.390140010639928, 'subsample': 1.0}\n",
    "xgb_params = {'learning_rate':0.05,'n_estimators':1000,'max_depth':5,'min_child_weight':1,'gamma':0,\n",
    "                           'subsample':0.8,'colsample_bytree':0.8,'objective': 'binary:logistic','nthread':4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 5 objects that represent our 4 models\n",
    "rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\n",
    "et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n",
    "ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\n",
    "gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\n",
    "xgb =  SklearnHelper(clf=XGBClassifier, seed=SEED, params=xgb_params)\n",
    "#svc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)\n",
    "#nn = SklearnHelper(clf = MLPClassifier, seed = SEED, params = nn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is complete\n"
     ]
    }
   ],
   "source": [
    "# Create our OOF train and test predictions. These base results will be used as new features\n",
    "et_oof_train, et_oof_test = get_oof(et, X_train, y_train, X_test) # Extra Trees\n",
    "rf_oof_train, rf_oof_test = get_oof(rf,X_train, y_train, X_test) # Random Forest\n",
    "ada_oof_train, ada_oof_test = get_oof(ada, X_train, y_train, X_test) # AdaBoost \n",
    "gb_oof_train, gb_oof_test = get_oof(gb,X_train, y_train, X_test) # Gradient Boost\n",
    "xgb_oof_train,xgb_oof_test = get_oof(xgb,X_train, y_train, X_test) # Support Vector Classifier\n",
    "#nn_oof_train, nn_oof_test = get_oof(nn,X_train, y_train, X_test)\n",
    "print(\"Training is complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train,xgb_oof_train ), axis=1) #\n",
    "x_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test,xgb_oof_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLPClassifier( early_stopping=False, random_state=123).fit(x_train, y_train)#, early_stopping_rounds = 5, eval_set=[(x_test, y_test)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acurracy: \n",
      "0.7804740406320542\n",
      "test acurracy: \n",
      "0.768280123583934\n",
      "Matriz de confusión: \n",
      "[[1418  430]\n",
      " [  20   74]]\n",
      "Área bajo la curva: \n",
      "0.8546099290780143\n"
     ]
    }
   ],
   "source": [
    "preds = nn.predict(x_test)\n",
    "preds_prob = nn.predict_proba(x_test)[:,1]\n",
    "area_debajo_de_curva = roc_auc_score(y_test, preds_prob)\n",
    "matriz_de_confusion = confusion_matrix(y_test, preds)\n",
    "train_accuracy = accuracy_score(y_train, nn.predict(x_train))\n",
    "test_accuracy = accuracy_score(y_test, preds)\n",
    "print('train acurracy: ')\n",
    "print(train_accuracy)\n",
    "print('test acurracy: ')\n",
    "print(test_accuracy)\n",
    "print('Matriz de confusión: ')\n",
    "print(matriz_de_confusion)\n",
    "print('Área bajo la curva: ')\n",
    "print(area_debajo_de_curva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train_p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-17953afa3da3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0marea_debajo_de_curva\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmatriz_de_confusion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train acurracy: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train_p' is not defined"
     ]
    }
   ],
   "source": [
    "train_accuracy = accuracy_score(y_train_p, nn.predict(X_train_p))\n",
    "test_accuracy = accuracy_score(y_test_p, preds)\n",
    "area_debajo_de_curva = roc_auc_score(y_test_p, preds_prob)\n",
    "matriz_de_confusion = confusion_matrix(y_test_p, preds)\n",
    "print('train acurracy: ')\n",
    "print(train_accuracy)\n",
    "print('test acurracy: ')\n",
    "print(test_accuracy)\n",
    "print('Matriz de confusión: ')\n",
    "print(matriz_de_confusion)\n",
    "print('Área bajo la curva: ')\n",
    "print(area_debajo_de_curva)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uso como segundo Paso XGBOOST nuevamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg= XGBClassifier(learning_rate =0.05,n_estimators=1000,max_depth=5,min_child_weight=1,gamma=0,\n",
    "                           subsample=0.8,colsample_bytree=0.8,objective= 'binary:logistic',nthread=4,\n",
    "                            seed=27)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.321833\n",
      "Will train until validation_0-error hasn't improved in 5 rounds.\n",
      "[1]\tvalidation_0-error:0.249228\n",
      "[2]\tvalidation_0-error:0.237899\n",
      "[3]\tvalidation_0-error:0.246653\n",
      "[4]\tvalidation_0-error:0.247683\n",
      "[5]\tvalidation_0-error:0.26931\n",
      "[6]\tvalidation_0-error:0.263131\n",
      "[7]\tvalidation_0-error:0.255922\n",
      "Stopping. Best iteration:\n",
      "[2]\tvalidation_0-error:0.237899\n",
      "\n",
      "Matriz de confusión: \n",
      "[[1411  437]\n",
      " [  25   69]]\n",
      "Área bajo la curva: \n",
      "0.8505687574836511\n"
     ]
    }
   ],
   "source": [
    "xg_reg.fit(x_train, y_train, early_stopping_rounds = 5, eval_set=[(x_test, y_test)])\n",
    "preds = xg_reg.predict(x_test)\n",
    "preds_prob = xg_reg.predict_proba(x_test)[:,1]\n",
    "area_debajo_de_curva = roc_auc_score(y_test, preds_prob)\n",
    "matriz_de_confusion = confusion_matrix(y_test, preds)\n",
    "print('Matriz de confusión: ')\n",
    "print(matriz_de_confusion)\n",
    "print('Área bajo la curva: ')\n",
    "print(area_debajo_de_curva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
