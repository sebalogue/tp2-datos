{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt \n",
    "from datetime import timedelta\n",
    "%matplotlib inline\n",
    "\n",
    "#from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error, confusion_matrix,\\\n",
    "                            f1_score,precision_score,recall_score,classification_report,cohen_kappa_score\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seba\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (3,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "labels = pd.read_csv(\"../../data/labels_training_set.csv\")\n",
    "df = pd.read_csv(\"../../data/events_up_to_01062018.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_persons = pd.read_csv(\"../../data/trocafone_kaggle_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df[\"month\"] = df[\"timestamp\"].dt.month\n",
    "df[\"day\"] = df[\"timestamp\"].dt.day\n",
    "df['dayofyear'] = df['timestamp'].dt.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('../../data/labels_training_set.csv') #las personas de las cuales tengo Info\n",
    "personas =pd.read_csv('../../data/trocafone_kaggle_test.csv') #las personas a las que le tengo que predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.read_csv('../features_csv/features_finales_seba.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datos para entrenar\n",
    "\n",
    "labels_f = pd.merge( df_features, labels, on = 'person', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "todas_las_personas = df[[\"person\"]].drop_duplicates(subset=\"person\")\n",
    "df_con_labels = pd.merge(df, labels, on='person', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = labels_f[['label']]\n",
    "X = labels_f.drop(columns=['person', 'label'])\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state= 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prueba_svc(X, y, pesos={0: 1, 1: 20}):\n",
    "    model = SVC(C=0.5, kernel='linear', probability=True, class_weight=pesos)\n",
    "      \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y['label'], test_size= 0.2, random_state= 123)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    preds_prob = model.predict_proba(X_test)[:,1]\n",
    "    train_accuracy = accuracy_score(y_train, model.predict(X_train))\n",
    "    test_accuracy = accuracy_score(y_test, preds)\n",
    "    area_debajo_de_curva = roc_auc_score(y_test, preds_prob)\n",
    "    matriz_de_confusion = confusion_matrix(y_test, preds)\n",
    "    print('train acurracy: ', train_accuracy)\n",
    "    print('test acurracy: ', test_accuracy)\n",
    "    print('Matriz de confusión: ')\n",
    "    print(matriz_de_confusion)\n",
    "    print('Área bajo la curva: ', area_debajo_de_curva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prueba_random_forest(X, y, pesos={0: 1, 1: 15}):\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0, class_weight = pesos)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y['label'], test_size= 0.2, random_state= 123)\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    preds_prob = clf.predict_proba(X_test)[:,1]\n",
    "    train_accuracy = accuracy_score(y_train, clf.predict(X_train))\n",
    "    test_accuracy = accuracy_score(y_test, preds)\n",
    "    area_debajo_de_curva = roc_auc_score(y_test, preds_prob)\n",
    "    matriz_de_confusion = confusion_matrix(y_test, preds)\n",
    "    print('train acurracy: ', train_accuracy)\n",
    "    print('test acurracy: ', test_accuracy)\n",
    "    print('Matriz de confusión: ')\n",
    "    print(matriz_de_confusion)\n",
    "    print('Área bajo la curva: ', area_debajo_de_curva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prueba_decision_tree(X, y, pesos={0: 1, 1: 15}):\n",
    "    model = DecisionTreeClassifier(max_depth=5, random_state=123, class_weight = {0: 1, 1: 10}, min_samples_split=10)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y['label'], test_size= 0.2, random_state= 123)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    preds_prob = model.predict_proba(X_test)[:,1]\n",
    "    train_accuracy = accuracy_score(y_train, model.predict(X_train))\n",
    "    test_accuracy = accuracy_score(y_test, preds)\n",
    "    area_debajo_de_curva = roc_auc_score(y_test, preds_prob)\n",
    "    matriz_de_confusion = confusion_matrix(y_test, preds)\n",
    "    print('train acurracy: ', train_accuracy)\n",
    "    print('test acurracy: ', test_accuracy)\n",
    "    print('Matriz de confusión: ')\n",
    "    print(matriz_de_confusion)\n",
    "    print('Área bajo la curva: ', area_debajo_de_curva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prueba_bagging_classifier(X, y, pesos={0: 1, 1: 15}):\n",
    "    model = BaggingClassifier(DecisionTreeClassifier(max_depth=5, random_state=0, class_weight = {0: 1, 1: 10}))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y['label'], test_size= 0.2, random_state= 123)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    preds_prob = model.predict_proba(X_test)[:,1]\n",
    "    train_accuracy = accuracy_score(y_train, model.predict(X_train))\n",
    "    test_accuracy = accuracy_score(y_test, preds)\n",
    "    area_debajo_de_curva = roc_auc_score(y_test, preds_prob)\n",
    "    matriz_de_confusion = confusion_matrix(y_test, preds)\n",
    "    print('train acurracy: ', train_accuracy)\n",
    "    print('test acurracy: ', test_accuracy)\n",
    "    print('Matriz de confusión: ')\n",
    "    print(matriz_de_confusion)\n",
    "    print('Área bajo la curva: ', area_debajo_de_curva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prueba_ada_boost(X, y):\n",
    "    model = AdaBoostClassifier( learning_rate = 0.1, n_estimators = 100)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y['label'], test_size= 0.2, random_state= 123)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    preds_prob = model.predict_proba(X_test)[:,1]\n",
    "    train_accuracy = accuracy_score(y_train, model.predict(X_train))\n",
    "    test_accuracy = accuracy_score(y_test, preds)\n",
    "    area_debajo_de_curva = roc_auc_score(y_test, preds_prob)\n",
    "    matriz_de_confusion = confusion_matrix(y_test, preds)\n",
    "    print('train acurracy: ', train_accuracy)\n",
    "    print('test acurracy: ', test_accuracy)\n",
    "    print('Matriz de confusión: ')\n",
    "    print(matriz_de_confusion)\n",
    "    print('Área bajo la curva: ', area_debajo_de_curva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prueba_xgboost(X, y):\n",
    "    model = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                colsample_bytree = 0.8, learning_rate = 0.1,\n",
    "                max_depth = 5, n_estimators = 6, scale_pos_weight = 7, min_child_weight=15)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y['label'], test_size= 0.2, random_state= 123)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    preds_prob = model.predict_proba(X_test)[:,1]\n",
    "    train_accuracy = accuracy_score(y_train, model.predict(X_train))\n",
    "    test_accuracy = accuracy_score(y_test, preds)\n",
    "    area_debajo_de_curva = roc_auc_score(y_test, preds_prob)\n",
    "    matriz_de_confusion = confusion_matrix(y_test, preds)\n",
    "    print('train acurracy: ', train_accuracy)\n",
    "    print('test acurracy: ', test_accuracy)\n",
    "    print('Matriz de confusión: ')\n",
    "    print(matriz_de_confusion)\n",
    "    print('Área bajo la curva: ', area_debajo_de_curva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prueba_xgboost_oversampling(df, cant_unos):\n",
    "   \n",
    "    df_train = pd.merge(df, labels, on=\"person\", how=\"inner\")\\\n",
    "            .drop(columns = ['person'])\n",
    "    \n",
    "    # Separamos la variable a predecir\n",
    "\n",
    "    X, y = df_train.iloc[:,:-1],df_train.iloc[:,-1]\n",
    "\n",
    "\n",
    "    #Creamos set de entrenamiento y test\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "    \n",
    "    cant_ceros = np.sum(y_train == 0)\n",
    "    \n",
    "    ros = RandomOverSampler(random_state=2, sampling_strategy = {0: cant_ceros, 1: cant_unos})\n",
    "    X_train_res, y_train_res = ros.fit_sample(X_train, y_train)\n",
    "    X_train_res_df = pd.DataFrame(X_train_res)\n",
    "    X_train_res_df.columns = X_test.columns\n",
    "\n",
    "    #Instanciamos el regresor de XGBoost\n",
    "\n",
    "    xg_reg = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                colsample_bytree = 0.8, learning_rate = 0.1,\n",
    "                max_depth = 5, n_estimators = 6, scale_pos_weight = 7, min_child_weight=15)\n",
    "\n",
    "    #Entrenamos\n",
    "\n",
    "    xg_reg.fit(X_train_res_df, y_train_res)\n",
    "\n",
    "    # Predecimos\n",
    "\n",
    "    preds = xg_reg.predict(X_test)\n",
    "    proba = xg_reg.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # Vemos que onda\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train_res, xg_reg.predict(X_train_res_df))\n",
    "    test_accuracy = accuracy_score(y_test, preds)\n",
    "    matriz_de_confusion = confusion_matrix(y_test, preds)\n",
    "    area_debajo_de_curva = roc_auc_score(y_test, proba)\n",
    "\n",
    "    print(\"Train accuracy: \", train_accuracy)\n",
    "    print(\"Test acuracy: \", test_accuracy)\n",
    "    print(\"ROC auc score: \", area_debajo_de_curva)\n",
    "    print(\"Confusion matrix: \")\n",
    "    print(matriz_de_confusion)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acurracy:  0.8790161612259352\n",
      "test acurracy:  0.872263713623487\n",
      "Matriz de confusión: \n",
      "[[3264  421]\n",
      " [  75  123]]\n",
      "Área bajo la curva:  0.8727457752559515\n"
     ]
    }
   ],
   "source": [
    "prueba_xgboost(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(label    782\n",
       " dtype: int64, label    14749\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "np.sum(y_train == 1), np.sum(y_train == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.8603744648220334\n",
      "Test acuracy:  0.8599021375225341\n",
      "ROC auc score:  0.8728718665625043\n",
      "Confusion matrix: \n",
      "[[3210  475]\n",
      " [  69  129]]\n"
     ]
    }
   ],
   "source": [
    "prueba_xgboost_oversampling(df_features, 900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acurracy:  0.9495847015646127\n",
      "test acurracy:  0.950296162760752\n",
      "Matriz de confusión: \n",
      "[[3685    0]\n",
      " [ 193    5]]\n",
      "Área bajo la curva:  0.8706008524868769\n"
     ]
    }
   ],
   "source": [
    "prueba_ada_boost(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acurracy:  0.8430236301590368\n",
      "test acurracy:  0.8411022405356683\n",
      "Matriz de confusión: \n",
      "[[3133  552]\n",
      " [  65  133]]\n",
      "Área bajo la curva:  0.8666673519455065\n"
     ]
    }
   ],
   "source": [
    "prueba_bagging_classifier(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acurracy:  0.8181057240357994\n",
      "test acurracy:  0.8174092196755086\n",
      "Matriz de confusión: \n",
      "[[3035  650]\n",
      " [  59  139]]\n",
      "Área bajo la curva:  0.813254663322506\n"
     ]
    }
   ],
   "source": [
    "prueba_decision_tree(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acurracy:  0.8207456055630674\n",
      "test acurracy:  0.8241050733968581\n",
      "Matriz de confusión: \n",
      "[[3049  636]\n",
      " [  47  151]]\n",
      "Área bajo la curva:  0.8682174526814962\n"
     ]
    }
   ],
   "source": [
    "prueba_random_forest(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensamble con promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Área bajo la curva:  0.8756534133739019\n"
     ]
    }
   ],
   "source": [
    "#Prueba con oversampling\n",
    "\n",
    "pesos={0: 1, 1: 15}\n",
    "model1 = RandomForestClassifier(n_estimators=16, max_depth=5, random_state=0, class_weight = pesos)\n",
    "\n",
    "model2 = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                colsample_bytree = 0.8, learning_rate = 0.1,\n",
    "                max_depth = 5, n_estimators = 6, scale_pos_weight = 7, min_child_weight=15)\n",
    "\n",
    "\n",
    "model3 = DecisionTreeClassifier(max_depth=5, random_state=123, class_weight = {0: 1, 1: 10})\n",
    "model4=  BaggingClassifier(DecisionTreeClassifier(max_depth=5, random_state=0, class_weight = {0: 1, 1: 10}))\n",
    "model5 = AdaBoostClassifier( learning_rate = 0.1, n_estimators = 100)\n",
    "\n",
    "df_train = pd.merge(df_features, labels, on=\"person\", how=\"inner\")\\\n",
    "            .drop(columns = ['person'])\n",
    "    \n",
    "# Separamos la variable a predecir\n",
    "\n",
    "X, y = df_train.iloc[:,:-1],df_train.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state= 123)\n",
    "\n",
    "cant_unos = 900\n",
    "ros = RandomOverSampler(random_state=2, sampling_strategy = {0: np.sum(y_train == 0), 1: cant_unos})\n",
    "X_train_res, y_train_res = ros.fit_sample(X_train, y_train)\n",
    "X_train_res_df = pd.DataFrame(X_train_res)\n",
    "X_train_res_df.columns = X_test.columns\n",
    "\n",
    "model1.fit(X_train, y_train)\n",
    "model2.fit(X_train_res_df, y_train_res)\n",
    "model3.fit(X_train, y_train)\n",
    "model4.fit(X_train_res_df, y_train_res)\n",
    "model5.fit(X_train_res_df, y_train_res)\n",
    "\n",
    "pred1=model1.predict_proba(X_test)[:,1]\n",
    "pred2=model2.predict_proba(X_test)[:,1]\n",
    "pred3=model3.predict_proba(X_test)[:,1]\n",
    "pred4 = model4.predict_proba(X_test)[:,1]\n",
    "pred5 = model5.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "preds_prob = (pred1 + pred2 + pred5)/3\n",
    "\n",
    "area_debajo_de_curva = roc_auc_score(y_test, preds_prob)\n",
    "print('Área bajo la curva: ', area_debajo_de_curva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esto seria para kaggle\n",
    "\n",
    "pesos={0: 1, 1: 15}\n",
    "model1 = RandomForestClassifier(n_estimators=16, max_depth=5, random_state=0, class_weight = pesos)\n",
    "\n",
    "model2 = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                colsample_bytree = 0.8, learning_rate = 0.1,\n",
    "                max_depth = 5, n_estimators = 6, scale_pos_weight = 7, min_child_weight=15)\n",
    "\n",
    "df_train = pd.merge(df_features, labels, on=\"person\", how=\"inner\")\\\n",
    "            .drop(columns = ['person'])\n",
    "\n",
    "datos = pd.merge(df_features, personas, on='person', how='inner').filter(items= list(df_train.columns) + ['person'])\n",
    "df_final_sin_persons = datos.drop(columns='person')\n",
    "    \n",
    "# Separamos la variable a predecir\n",
    "\n",
    "X, y = df_train.iloc[:,:-1],df_train.iloc[:,-1]\n",
    "\n",
    "cant_unos = 1098\n",
    "ros = RandomOverSampler(random_state=2, sampling_strategy = {0: np.sum(y == 0), 1: cant_unos})\n",
    "X_res, y_res = ros.fit_sample(X, y)\n",
    "X_res_df = pd.DataFrame(X_res)\n",
    "X_res_df.columns = X.columns\n",
    "\n",
    "model3 = DecisionTreeClassifier(max_depth=5, random_state=123, class_weight = {0: 1, 1: 10})\n",
    "model4=  BaggingClassifier(DecisionTreeClassifier(max_depth=5, random_state=0, class_weight = {0: 1, 1: 10}))\n",
    "model5 = AdaBoostClassifier( learning_rate = 0.1, n_estimators = 100)\n",
    "\n",
    "model1.fit(X, y)\n",
    "model2.fit(X_res_df, y_res)\n",
    "model3.fit(X, y)\n",
    "model4.fit(X_res_df, y_res)\n",
    "model5.fit(X_res_df, y_res)\n",
    "\n",
    "pred1 = pd.Series(model1.predict_proba(df_final_sin_persons)[:,1])\n",
    "pred2 = pd.Series(model2.predict_proba(df_final_sin_persons)[:,1])\n",
    "pred5 = pd.Series(model5.predict_proba(df_final_sin_persons)[:,1])\n",
    "\n",
    "preds_prob = (pred1 + pred2 + pred5)/3\n",
    "\n",
    "predicciones = datos[['person']]\n",
    "predicciones['label'] = preds_prob\n",
    "predicciones.to_csv('predicciones.csv', encoding='utf-8', index=False)\n",
    "\n",
    "print(predicciones.shape[0] == 19415)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaggle_promedio(df):\n",
    "    \n",
    "    # Separamos la variable a predecir\n",
    "\n",
    "    df_train = pd.merge(df, labels, on=\"person\", how=\"inner\")\\\n",
    "            .drop(columns = ['person'])\n",
    "    \n",
    "    X, y = df_train.iloc[:,:-1],df_train.iloc[:,-1]\n",
    "    \n",
    "    datos = pd.merge(df, personas, on='person', how='inner').filter(items= list(df_train.columns) + ['person'])\n",
    "    df_final_sin_persons = datos.drop(columns='person')\n",
    "\n",
    "    # Entreno\n",
    "    pesos={0: 1, 1: 15}\n",
    "\n",
    "    model1 = RandomForestClassifier(n_estimators=16, max_depth=5, random_state=0, class_weight = pesos)\n",
    "\n",
    "    model2 = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                colsample_bytree = 0.8, learning_rate = 0.1,\n",
    "                max_depth = 5, n_estimators = 6, scale_pos_weight = 7, min_child_weight=15)\n",
    "\n",
    "    model1.fit(X,y)\n",
    "    model2.fit(X,y)\n",
    "\n",
    "\n",
    "    pred1 = pd.Series(model1.predict_proba(df_final_sin_persons)[:,1])\n",
    "    pred2 = pd.Series(model2.predict_proba(df_final_sin_persons)[:,1])\n",
    "\n",
    "    proba=(0.8*pred2 + 0.2*pred1)\n",
    "    predicciones = datos[['person']]\n",
    "    predicciones['label'] = proba\n",
    "    predicciones.to_csv('predicciones.csv', encoding='utf-8', index=False)\n",
    "\n",
    "    print(predicciones.shape[0] == 19415)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaggle_oversample(df):\n",
    "    \n",
    "    # Separamos la variable a predecir\n",
    "\n",
    "    df_train = pd.merge(df, labels, on=\"person\", how=\"inner\")\\\n",
    "            .drop(columns = ['person'])\n",
    "    \n",
    "    X, y = df_train.iloc[:,:-1],df_train.iloc[:,-1]\n",
    "    \n",
    "    cant_unos = np.sum(y == 1) + 91\n",
    "    cant_ceros = np.sum(y == 0)\n",
    "    ros = RandomOverSampler(random_state=2, sampling_strategy = {0: cant_ceros, 1: cant_unos})\n",
    "    X_ro, y_ro = ros.fit_sample(X, y)\n",
    "    X_ro = pd.DataFrame(X_ro)\n",
    "    X_ro.columns = X.columns\n",
    "    \n",
    "    datos = pd.merge(df, personas, on='person', how='inner').filter(items= list(df_train.columns) + ['person'])\n",
    "    df_final_sin_persons = datos.drop(columns='person')\n",
    "\n",
    "    # Entreno\n",
    "   \n",
    "    xg_reg = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                colsample_bytree = 0.8, learning_rate = 0.1,\n",
    "                max_depth = 5, n_estimators = 6, scale_pos_weight = 7, min_child_weight=15)\n",
    "\n",
    "    xg_reg.fit(X_ro, y_ro)\n",
    "\n",
    "    proba = pd.Series(xg_reg.predict_proba(df_final_sin_persons)[:,1])\n",
    "\n",
    "    predicciones = datos[['person']]\n",
    "    predicciones['label'] = proba\n",
    "    predicciones.to_csv('predicciones.csv', encoding='utf-8', index=False)\n",
    "\n",
    "    print(predicciones.shape[0] == 19415)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaggle(df):\n",
    "    \n",
    "    # Separamos la variable a predecir\n",
    "\n",
    "    df_train = pd.merge(df, labels, on=\"person\", how=\"inner\")\\\n",
    "            .drop(columns = ['person'])\n",
    "    \n",
    "    X, y = df_train.iloc[:,:-1],df_train.iloc[:,-1]\n",
    "    \n",
    "    datos = pd.merge(df, personas, on='person', how='inner').filter(items= list(df_train.columns) + ['person'])\n",
    "    df_final_sin_persons = datos.drop(columns='person')\n",
    "\n",
    "    # Entreno\n",
    "   \n",
    "    xg_reg = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                colsample_bytree = 0.8, learning_rate = 0.1,\n",
    "                max_depth = 5, n_estimators = 6, scale_pos_weight = 7, min_child_weight=15)\n",
    "\n",
    "    xg_reg.fit(X, y)\n",
    "\n",
    "    proba = pd.Series(xg_reg.predict_proba(df_final_sin_persons)[:,1])\n",
    "\n",
    "    predicciones = datos[['person']]\n",
    "    predicciones['label'] = proba\n",
    "    predicciones.to_csv('predicciones.csv', encoding='utf-8', index=False)\n",
    "\n",
    "    print(predicciones.shape[0] == 19415)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_promedio(df_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
