{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplico Algoritmos y busco mejorarlos ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import SCORERS\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "tf = pd.read_csv('featuresMagui.csv') #mis features \n",
    "tf_seba = pd.read_csv('features_seba.csv') #los features de Seba\n",
    "labels = pd.read_csv('../data/labels_training_set.csv') #las personas de las cuales tengo Info\n",
    "personas =pd.read_csv('../data/trocafone_kaggle_test.csv') #las personas a las que le tengo que predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datos a entregar\n",
    "datos = pd.merge(tf, tf_seba, on = 'person', how = 'inner')\n",
    "datos = pd.merge(personas, datos, on = 'person', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datos para entrenar\n",
    "labels_f = pd.merge(tf,tf_seba, on = 'person',how = 'inner')\n",
    "labels_f = pd.merge(labels, labels_f, on = 'person', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columnas a usar. Saco personingreso Por publicidad, lead, promedio por dia y promedio por mes (pq están rotos). \n",
    "columnas = ['esNuevo', 'cantidadDeVisitas',\n",
    "       'deviceSmartphone', 'deviceComputer',\n",
    "       'deviceTablet', 'deviceUnknown', 'cantVisitasAVP', 'maniana', 'tarde',\n",
    "       'noche', 'madrugada', 'cantDirect', 'cantEmail', 'cantOrganic',\n",
    "       'cantPaid', 'cantReferral', 'cantSocial', 'cantUnknown',\n",
    "       'cantVisitasDomingo', 'cantVisitasLunes', 'cantVisitasMartes',\n",
    "       'cantVisitasMiercoles', 'cantVisitasJueves', 'cantVisitasViernes',\n",
    "       'cantVisitasSabado', 'cantidadDeEventos', 'cantAdCampaignHit',\n",
    "       'cantBandListing', 'cantCheckout', 'cantGenericListing', 'cantLead',\n",
    "       'cantSearchEngineHit', 'cantSearchedProducts', 'cantStaticpage',\n",
    "       'cantViewedProduct', 'cantVisitedSite', 'visitaALaManiana',\n",
    "       'visitaALaNoche', 'visitaALaTarde', 'visitaALaMadrugada', 'canalDirect',\n",
    "       'canalEmail', 'canalOrganic', 'canalPaid', 'canalRefferal',\n",
    "       'canalSocial', 'adCampaignHit', 'bandListing', 'checkout',\n",
    "       'genericListing', 'searchEngineHit', 'searchedProducts', 'staticpage',\n",
    "       'viewedProduct', 'visitedSite', 'ad campaign hit mes 5',\n",
    "       'brand listing mes 5', 'checkout mes 5', 'conversion mes 5',\n",
    "       'generic listing mes 5', 'lead mes 5', 'search engine hit mes 5',\n",
    "       'searched products mes 5', 'staticpage mes 5', 'viewed product mes 5',\n",
    "       'visited site mes 5', 'dias_hasta_ultimo', 'ad campaign hit mes 4',\n",
    "       'brand listing mes 4', 'checkout mes 4', 'conversion mes 4',\n",
    "       'generic listing mes 4', 'lead mes 4', 'search engine hit mes 4',\n",
    "       'searched products mes 4', 'staticpage mes 4', 'viewed product mes 4',\n",
    "       'visited site mes 4', 'distan_dias', 'nuevo_mes_5', 'cant_dias_dist',\n",
    "       'modelos_dist',\n",
    "       'mes_primer_entrada']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defino Xgboost en un solo, lo que hace es entrenar predecir e imprimir las predicciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost(xg_reg_p, X_train_p, X_test_p, y_train_p, y_test_p, X_p, y_p):\n",
    "    xg_reg_p.fit(X_train_p,y_train_p)\n",
    "    preds = xg_reg.predict(X_test_p)\n",
    "    preds_prob = xg_reg.predict_proba(X_test_p)[:,1]\n",
    "    train_accuracy = accuracy_score(y_train_p, xg_reg_p.predict(X_train_p))\n",
    "    test_accuracy = accuracy_score(y_test_p, preds)\n",
    "    area_debajo_de_curva = roc_auc_score(y_test_p, preds_prob)\n",
    "    matriz_de_confusion = confusion_matrix(y_test_p, preds)\n",
    "    print('train acurracy: ')\n",
    "    print(train_accuracy)\n",
    "    print('test acurracy: ')\n",
    "    print(test_accuracy)\n",
    "    print('Matriz de confusión: ')\n",
    "    print(matriz_de_confusion)\n",
    "    print('Área bajo la curva: ')\n",
    "    print(area_debajo_de_curva)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Por un lado tengo los features y por otro los labels. \n",
    "labels_f.reset_index()\n",
    "X = labels_f.loc[:,columnas]\n",
    "y = labels_f.loc[:,'label_x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplico XGboost solo, es decir, **sin ninguna mejora **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acurracy: \n",
      "0.998282967032967\n",
      "test acurracy: \n",
      "0.9449021627188465\n",
      "Matriz de confusión: \n",
      "[[1832   16]\n",
      " [  91    3]]\n",
      "Área bajo la curva: \n",
      "0.7859733351754629\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.1, random_state= 123)\n",
    "\n",
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "xg_reg = xgb.XGBClassifier(learning_rate =0.1,n_estimators=1000,max_depth=5,min_child_weight=1,gamma=0,subsample=0.8,\n",
    "                               colsample_bytree=0.8,objective= 'binary:logistic',nthread=4,scale_pos_weight=1,\n",
    "                               seed=27)\n",
    "    \n",
    "xgboost(xg_reg,X_train, X_test, y_train, y_test, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos de la matriz de confusión son **ALARMANTES** ya que predice mal los que compran. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trato de centrar los datos con el **escalador estandar** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype bool, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DataConversionWarning: Data with input dtype bool, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype bool, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acurracy: \n",
      "0.998841027622175\n",
      "test acurracy: \n",
      "0.9451455060520216\n",
      "Matriz de confusión: \n",
      "[[3662   30]\n",
      " [ 183    8]]\n",
      "Área bajo la curva: \n",
      "0.8057034312196173\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "\n",
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "xg_reg = xgb.XGBClassifier(learning_rate =0.1,n_estimators=1000,max_depth=5,min_child_weight=1,gamma=0,subsample=0.8,\n",
    "                               colsample_bytree=0.8,objective= 'binary:logistic',nthread=4,scale_pos_weight=1,\n",
    "                               seed=27)\n",
    "\n",
    "xgboost(xg_reg, X_train_transformed, X_test_transformed, y_train, y_test, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trato de centrar los datos con el escalador **MaxAbsScaler **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acurracy: \n",
      "0.998841027622175\n",
      "test acurracy: \n",
      "0.943857841874839\n",
      "Matriz de confusión: \n",
      "[[3657   35]\n",
      " [ 183    8]]\n",
      "Área bajo la curva: \n",
      "0.8074873931466365\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "scaler = preprocessing.MaxAbsScaler().fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "\n",
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "xg_reg = xgb.XGBClassifier(learning_rate =0.1,n_estimators=1000,max_depth=5,min_child_weight=1,gamma=0,subsample=0.8,\n",
    "                               colsample_bytree=0.8,objective= 'binary:logistic',nthread=4,scale_pos_weight=1,\n",
    "                               seed=27)\n",
    "\n",
    "xgboost(xg_reg, X_train_transformed, X_test_transformed, y_train, y_test, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El escalador MaxAbsScales es mejor que el estandar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Trato de usar Cross-Validation ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['explained_variance', 'r2', 'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'accuracy', 'roc_auc', 'balanced_accuracy', 'average_precision', 'neg_log_loss', 'brier_score_loss', 'adjusted_rand_score', 'homogeneity_score', 'completeness_score', 'v_measure_score', 'mutual_info_score', 'adjusted_mutual_info_score', 'normalized_mutual_info_score', 'fowlkes_mallows_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Estos son los scores que se pueden usar\n",
    "SCORERS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['estimator', 'fit_time', 'score_time', 'test_roc_auc', 'train_roc_auc']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring = ['roc_auc']\n",
    "\n",
    "xg_reg = xgb.XGBClassifier(learning_rate =0.1,n_estimators=1000,max_depth=5,min_child_weight=1,gamma=0,\n",
    "                           subsample=0.8,colsample_bytree=0.8,objective= 'binary:logistic',nthread=4,\n",
    "                         scale_pos_weight=1, seed=27)\n",
    "\n",
    "scaler = preprocessing.MaxAbsScaler().fit(X)\n",
    "X_scaled= scaler.transform(X)\n",
    "data_dmatrix = xgb.DMatrix(data=X_scaled,label=y)\n",
    "scores = cross_validate(xg_reg, X_scaled, y, scoring=scoring,\n",
    "                        cv= 7, return_train_score=True, return_estimator=True)\n",
    "sorted(scores.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.78670816 0.80803232 0.7859475  0.77145163 0.79538007 0.83433482\n",
      " 0.8151294 ]\n",
      "train acurracy: \n",
      "0.998841027622175\n",
      "test acurracy: \n",
      "0.943857841874839\n",
      "Matriz de confusión: \n",
      "[[3657   35]\n",
      " [ 183    8]]\n",
      "Área bajo la curva: \n",
      "0.8074873931466365\n"
     ]
    }
   ],
   "source": [
    "print(scores['test_roc_auc'])\n",
    "\n",
    "#¿Lo pruebo ya entrenado?\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "scaler = preprocessing.MaxAbsScaler().fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "\n",
    "xgboost(xg_reg, X_train_transformed, X_test_transformed, y_train, y_test, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Da lo mismo???? ¿esto aplicando vien el cross validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En función de los datos obtenidos de la matriz de confusión, nos dimos cuenta que estaba dando muchos falsos negativos de los que compraron. Por lo tanto, recurriremos a una técnica de **UNDER SAMPLING** y luego a otra técnica de **OVER SAMPLING** y luego compararemos los resultados obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18434 980\n"
     ]
    }
   ],
   "source": [
    "#UNDER SAMPLING\n",
    "#cuánta información hay de cada clase \n",
    "count_class_0, count_class_1 = labels_f.label_x.value_counts()\n",
    "print(count_class_0, count_class_1)\n",
    "# divido por clase la información\n",
    "label_0 = labels_f[labels_f['label_x'] == 0]\n",
    "label_1 = labels_f[labels_f['label_x'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acurracy: \n",
      "1.0\n",
      "test acurracy: \n",
      "0.8316326530612245\n",
      "Matriz de confusión: \n",
      "[[82 19]\n",
      " [14 81]]\n",
      "Área bajo la curva: \n",
      "0.8820218863991661\n"
     ]
    }
   ],
   "source": [
    "label_0_under = label_0.sample(count_class_1)\n",
    "label_f_under = pd.concat([label_0_under, label_1], axis=0)\n",
    "\n",
    "X_u = label_f_under.loc[:,columnas]\n",
    "y_u = label_f_under.loc[:,['label_x']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_u, y_u, test_size= 0.1, random_state= 123)\n",
    "\n",
    "xg_reg = xgb.XGBClassifier(learning_rate =0.1,n_estimators=1000,max_depth=5,min_child_weight=1,gamma=0,\n",
    "                           subsample=0.8,colsample_bytree=0.8,objective= 'binary:logistic',nthread=4,\n",
    "                            scale_pos_weight=1, seed=27)\n",
    "\n",
    "scaler = preprocessing.MaxAbsScaler().fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "\n",
    "xgboost(xg_reg,X_train, X_test, y_train, y_test, X_u, y_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acurracy: \n",
      "0.9964437479280311\n",
      "test acurracy: \n",
      "0.9804719283970708\n",
      "Matriz de confusión: \n",
      "[[1742   72]\n",
      " [   0 1873]]\n",
      "Área bajo la curva: \n",
      "0.9987497137703959\n"
     ]
    }
   ],
   "source": [
    "#OVER SAMPLING\n",
    "label_0 = labels_f[labels_f['label_x'] == 0]\n",
    "label_1 = labels_f[labels_f['label_x'] == 1]\n",
    "\n",
    "label_1_over = label_1.sample(count_class_0, replace=True)\n",
    "label_f_over = pd.concat([label_0, label_1_over], axis=0)\n",
    "\n",
    "X_o = label_f_over.loc[:,columnas]\n",
    "y_o = label_f_over.loc[:,['label_x']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_o, y_o, test_size= 0.1, random_state= 123)\n",
    "\n",
    "xg_reg = xgb.XGBClassifier(learning_rate =0.1,n_estimators=1000,max_depth=5,min_child_weight=1,gamma=0,\n",
    "                           subsample=0.8,colsample_bytree=0.8,objective= 'binary:logistic',nthread=4,\n",
    "                            scale_pos_weight=1, seed=27)\n",
    "\n",
    "scaler = preprocessing.MaxAbsScaler().fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "\n",
    "xgboost(xg_reg,X_train, X_test, y_train, y_test, X_o, y_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver usando **Under sampling, mejoraron los resultados** teniendo en cuenta la matriz de confusión, pero **Over sampling** es mucho mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uso otra manera de hacer over\\under sampling que es mediante la librería **imbalanced-learn** de python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFO: \n",
    "\n",
    "For example, we can cluster the records of the majority class, and do the under-sampling by removing records from each cluster, thus seeking to preserve information. In over-sampling, instead of creating exact copies of the minority class records, we can introduce small variations into those copies, creating more diverse synthetic samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNDER SAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acurracy: \n",
      "0.9964437479280311\n",
      "test acurracy: \n",
      "0.9804719283970708\n",
      "Matriz de confusión: \n",
      "[[1742   72]\n",
      " [   0 1873]]\n",
      "Área bajo la curva: \n",
      "0.9987497137703959\n"
     ]
    }
   ],
   "source": [
    "rus = RandomUnderSampler(return_indices=True)\n",
    "\n",
    "#id_rus son los índices\n",
    "X_rus, y_rus, id_rus = rus.fit_sample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_o, y_o, test_size= 0.1, random_state= 123)\n",
    "\n",
    "xg_reg = xgb.XGBClassifier(learning_rate =0.1,n_estimators=1000,max_depth=5,min_child_weight=1,gamma=0,\n",
    "                           subsample=0.8,colsample_bytree=0.8,objective= 'binary:logistic',nthread=4,\n",
    "                            scale_pos_weight=1, seed=27)\n",
    "\n",
    "scaler = preprocessing.MaxAbsScaler().fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "\n",
    "xgboost(xg_reg,X_train, X_test, y_train, y_test, X_o, y_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OVER SAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acurracy: \n",
      "0.9964437479280311\n",
      "test acurracy: \n",
      "0.9804719283970708\n",
      "Matriz de confusión: \n",
      "[[1742   72]\n",
      " [   0 1873]]\n",
      "Área bajo la curva: \n",
      "0.9987497137703959\n"
     ]
    }
   ],
   "source": [
    "ros = RandomOverSampler()\n",
    "X_ros, y_ros = ros.fit_sample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_o, y_o, test_size= 0.1, random_state= 123)\n",
    "\n",
    "xg_reg = xgb.XGBClassifier(learning_rate =0.1,n_estimators=1000,max_depth=5,min_child_weight=1,gamma=0,\n",
    "                           subsample=0.8,colsample_bytree=0.8,objective= 'binary:logistic',nthread=4,\n",
    "                            scale_pos_weight=1, seed=27)\n",
    "\n",
    "scaler = preprocessing.MaxAbsScaler().fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "\n",
    "xgboost(xg_reg,X_train, X_test, y_train, y_test, X_o, y_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uso otra manera de hacer Undersample (que estuvo dando buenos resultados) con la siguiente técnica **Under-sampling: Tomek links **\n",
    "\n",
    "Info:\n",
    "\n",
    "Tomek links are pairs of very close instances, but of opposite classes. Removing the instances of the majority class of each pair increases the space between the two classes, facilitating the classification process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 19414 entries, 0 to 19413\n",
      "Data columns (total 83 columns):\n",
      "esNuevo                    19414 non-null int64\n",
      "cantidadDeVisitas          19414 non-null float64\n",
      "deviceSmartphone           19414 non-null int64\n",
      "deviceComputer             19414 non-null int64\n",
      "deviceTablet               19414 non-null int64\n",
      "deviceUnknown              19414 non-null int64\n",
      "cantVisitasAVP             19414 non-null float64\n",
      "maniana                    19414 non-null float64\n",
      "tarde                      19414 non-null float64\n",
      "noche                      19414 non-null float64\n",
      "madrugada                  19414 non-null float64\n",
      "cantDirect                 19414 non-null float64\n",
      "cantEmail                  19414 non-null float64\n",
      "cantOrganic                19414 non-null float64\n",
      "cantPaid                   19414 non-null float64\n",
      "cantReferral               19414 non-null float64\n",
      "cantSocial                 19414 non-null float64\n",
      "cantUnknown                19414 non-null float64\n",
      "cantVisitasDomingo         19414 non-null float64\n",
      "cantVisitasLunes           19414 non-null float64\n",
      "cantVisitasMartes          19414 non-null float64\n",
      "cantVisitasMiercoles       19414 non-null float64\n",
      "cantVisitasJueves          19414 non-null float64\n",
      "cantVisitasViernes         19414 non-null float64\n",
      "cantVisitasSabado          19414 non-null float64\n",
      "cantidadDeEventos          19414 non-null float64\n",
      "cantAdCampaignHit          19414 non-null float64\n",
      "cantBandListing            19414 non-null float64\n",
      "cantCheckout               19414 non-null float64\n",
      "cantGenericListing         19414 non-null float64\n",
      "cantLead                   19414 non-null float64\n",
      "cantSearchEngineHit        19414 non-null float64\n",
      "cantSearchedProducts       19414 non-null float64\n",
      "cantStaticpage             19414 non-null float64\n",
      "cantViewedProduct          19414 non-null float64\n",
      "cantVisitedSite            19414 non-null float64\n",
      "visitaALaManiana           19414 non-null int64\n",
      "visitaALaNoche             19414 non-null int64\n",
      "visitaALaTarde             19414 non-null int64\n",
      "visitaALaMadrugada         19414 non-null int64\n",
      "canalDirect                19414 non-null float64\n",
      "canalEmail                 19414 non-null float64\n",
      "canalOrganic               19414 non-null float64\n",
      "canalPaid                  19414 non-null float64\n",
      "canalRefferal              19414 non-null float64\n",
      "canalSocial                19414 non-null float64\n",
      "adCampaignHit              19414 non-null int64\n",
      "bandListing                19414 non-null int64\n",
      "checkout                   19414 non-null int64\n",
      "genericListing             19414 non-null int64\n",
      "searchEngineHit            19414 non-null int64\n",
      "searchedProducts           19414 non-null int64\n",
      "staticpage                 19414 non-null int64\n",
      "viewedProduct              19414 non-null int64\n",
      "visitedSite                19414 non-null int64\n",
      "ad campaign hit mes 5      18562 non-null float64\n",
      "brand listing mes 5        18562 non-null float64\n",
      "checkout mes 5             18562 non-null float64\n",
      "conversion mes 5           18562 non-null float64\n",
      "generic listing mes 5      18562 non-null float64\n",
      "lead mes 5                 18562 non-null float64\n",
      "search engine hit mes 5    18562 non-null float64\n",
      "searched products mes 5    18562 non-null float64\n",
      "staticpage mes 5           18562 non-null float64\n",
      "viewed product mes 5       18562 non-null float64\n",
      "visited site mes 5         18562 non-null float64\n",
      "dias_hasta_ultimo          19414 non-null int64\n",
      "ad campaign hit mes 4      19414 non-null float64\n",
      "brand listing mes 4        19414 non-null float64\n",
      "checkout mes 4             19414 non-null float64\n",
      "conversion mes 4           19414 non-null float64\n",
      "generic listing mes 4      19414 non-null float64\n",
      "lead mes 4                 19414 non-null float64\n",
      "search engine hit mes 4    19414 non-null float64\n",
      "searched products mes 4    19414 non-null float64\n",
      "staticpage mes 4           19414 non-null float64\n",
      "viewed product mes 4       19414 non-null float64\n",
      "visited site mes 4         19414 non-null float64\n",
      "distan_dias                19414 non-null int64\n",
      "nuevo_mes_5                19414 non-null float64\n",
      "cant_dias_dist             19414 non-null int64\n",
      "modelos_dist               19414 non-null int64\n",
      "mes_primer_entrada         19414 non-null int64\n",
      "dtypes: float64(60), int64(23)\n",
      "memory usage: 12.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#print(X.info())\n",
    "X['esNuevo']= X['esNuevo'].astype(int)\n",
    "X['adCampaignHit']= X['adCampaignHit'].astype(int)           \n",
    "X['bandListing']= X['bandListing'].astype(int)              \n",
    "X['checkout']=   X['checkout'].astype(int)                \n",
    "X['genericListing']=  X['genericListing'].astype(int)       \n",
    "X['searchEngineHit']= X['searchEngineHit'].astype(int)         \n",
    "X['searchedProducts']= X['searchedProducts'].astype(int)          \n",
    "X['staticpage']= X['staticpage'].astype(int)               \n",
    "X['viewedProduct']= X['viewedProduct'].astype(int)             \n",
    "X['visitedSite']= X['visitedSite'].astype(int)               \n",
    "X['deviceSmartphone']= X['deviceSmartphone'].astype(int)          \n",
    "X['deviceComputer']= X['deviceComputer'].astype(int)           \n",
    "X['deviceTablet']= X['deviceTablet'].astype(int)              \n",
    "X['deviceUnknown']= X['deviceUnknown'].astype(int)             \n",
    "X['visitaALaManiana']= X['visitaALaManiana'].astype(int)         \n",
    "X['visitaALaNoche']= X['visitaALaNoche'].astype(int)          \n",
    "X['visitaALaTarde']=  X['visitaALaTarde'].astype(int)           \n",
    "X['visitaALaMadrugada']= X['visitaALaMadrugada'].astype(int)\n",
    "print(X.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-9743fdb58bdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTomekLinks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'majority'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#print(X.info())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_tl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_tl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_o\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_o\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deprecate_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         self.sampling_strategy_ = check_sampling_strategy(\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_target_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicate_one_vs_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    757\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 573\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "tl = TomekLinks(return_indices=True, ratio='majority')\n",
    "#print(X.info())\n",
    "X_tl, y_tl, id_tl = tl.fit_sample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_o, y_o, test_size= 0.1, random_state= 123)\n",
    "\n",
    "xg_reg = xgb.XGBClassifier(learning_rate =0.1,n_estimators=1000,max_depth=5,min_child_weight=1,gamma=0,\n",
    "                           subsample=0.8,colsample_bytree=0.8,objective= 'binary:logistic',nthread=4,\n",
    "                            scale_pos_weight=1, seed=27)\n",
    "\n",
    "scaler = preprocessing.MaxAbsScaler().fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "\n",
    "xgboost(xg_reg,X_train, X_test, y_train, y_test, X_o, y_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NO SE PQ NO FUNCIONA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uso otra manera de hacer Under sample que es **Cluster Centroids**.\n",
    "\n",
    "INFO:\n",
    "\n",
    "This technique performs under-sampling by generating centroids based on clustering methods. The data will be previously grouped by similarity, in order to preserve information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-507c2523b07b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClusterCentroids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_cc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deprecate_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         self.sampling_strategy_ = check_sampling_strategy(\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_target_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicate_one_vs_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    757\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 573\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "cc = ClusterCentroids(ratio={0: 10})\n",
    "X_cc, y_cc = cc.fit_sample(X, y)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_o, y_o, test_size= 0.1, random_state= 123)\n",
    "\n",
    "xg_reg = xgb.XGBClassifier(learning_rate =0.1,n_estimators=1000,max_depth=5,min_child_weight=1,gamma=0,\n",
    "                           subsample=0.8,colsample_bytree=0.8,objective= 'binary:logistic',nthread=4,\n",
    "                            scale_pos_weight=1, seed=27)\n",
    "\n",
    "scaler = preprocessing.MaxAbsScaler().fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "\n",
    "xgboost(xg_reg,X_train, X_test, y_train, y_test, X_o, y_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MISMO ERROR QUE ANTES. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para hacer la entrega en Kaggle ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_f = datos.loc[:, [columnas]]\n",
    "scaler = preprocessing.MaxAbsScaler().fit(datos_f)\n",
    "datos_f = scaler.transform(datos_f)\n",
    "preds_entrega = xg_reg.predict_proba(datos_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds_entrega' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-10eedc1a2585>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds_entrega_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_entrega\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpreds_entrega_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds_entrega_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpredicciones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersonas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredicciones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds_entrega_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preds_entrega' is not defined"
     ]
    }
   ],
   "source": [
    "preds_entrega_df = pd.DataFrame(preds_entrega)\n",
    "preds_entrega_df = preds_entrega_df.loc[:,1]\n",
    "predicciones = personas\n",
    "predicciones['label'] = preds_entrega_df[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones.to_csv('predicciones.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
